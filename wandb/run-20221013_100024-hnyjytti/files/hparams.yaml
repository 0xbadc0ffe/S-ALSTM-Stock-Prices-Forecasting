data:
  datamodule:
    _target_: src.pl_data.datamodule.MyDataModule
    datasets:
      train:
        _target_: src.pl_data.dataset.MyDataset
        name: YourTrainDatasetName
        path: ${oc.env:YOUR_TRAIN_DATASET_PATH}
        train: true
      val:
      - _target_: src.pl_data.dataset.MyDataset
        name: YourValDatasetName
        path: ${oc.env:YOUR_VAL_DATASET_PATH}
        train: false
      test:
      - _target_: src.pl_data.dataset.MyDataset
        name: YourTestDatasetName
        path: ${oc.env:YOUR_TEST_DATASET_PATH}
        train: false
    num_workers:
      train: 1#8
      val: 1#4
      test: 1#4
    batch_size:
      train: 1
      val: 1
      test: 1
logging:
  val_check_interval: 1.0
  progress_bar_refresh_rate: 20
  wandb:
    project: nn-template
    entity: null
    log_model: true
    mode: online
  wandb_watch:
    log: all
    log_freq: 100
  lr_monitor:
    logging_interval: step
    log_momentum: false
model:
  _target_: src.pl_modules.model.MyModel
  num_layers: 1
  dropout_prob: 0.2
  look_back: 30
  input_dim: 8
  hidden_dim: 12
optim:
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
    betas:
    - 0.9
    - 0.999
    eps: 1.0e-08
    weight_decay: 0
  use_lr_scheduler: true
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 10
    T_mult: 2
    eta_min: 0
    last_epoch: -1
    verbose: true
train:
  deterministic: false
  random_seed: 42
  pl_trainer:
    fast_dev_run: false
    gpus: 1
    precision: 32
    max_steps: 10000
    accumulate_grad_batches: 1
    num_sanity_val_steps: 2
    gradient_clip_val: 10.0
  monitor_metric: val_loss
  monitor_metric_mode: min
  early_stopping:
    patience: 42
    verbose: false
  model_checkpoints:
    save_top_k: 2
    verbose: false
core:
  version: 0.0.1
  tags:
  - mytag
